{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c35c41f4",
   "metadata": {},
   "source": [
    "# Deep Learning vs Statistical Models for VaR Prediction\n",
    "## MENA Region Stock Market Analysis\n",
    "\n",
    "This notebook implements and compares deep learning models (ANN, LSTM, CNN) with statistical models (ARIMA, SARIMA) for predicting Value-at-Risk in MENA stock indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "f001d913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# TensorFlow/Keras imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Statsmodels for ARIMA/SARIMA\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9c03f36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ DataPreprocessor class created!\n"
     ]
    }
   ],
   "source": [
    "# ==================== DATA LOADING & PREPROCESSING ====================\n",
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(self, lookback=60):\n",
    "        self.lookback = lookback\n",
    "        self.scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        self.data_stats = {}\n",
    "    \n",
    "    def load_data(self, filepath):\n",
    "        \"\"\"Load CSV data and handle formatting\"\"\"\n",
    "        df = pd.read_csv(filepath)\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        df = df.sort_values('Date').reset_index(drop=True)\n",
    "        df['Price'] = pd.to_numeric(df['Price'].str.replace(',', ''), errors='coerce')\n",
    "        return df\n",
    "    \n",
    "    def handle_missing_values(self, df, method='forward_fill'):\n",
    "        \"\"\"\n",
    "        Methods:\n",
    "        - 'forward_fill': Use last known price (default, best for market holidays/weekends)\n",
    "        - 'interpolate': Linear interpolation between values\n",
    "        - 'drop': Remove rows with missing values\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        missing_before = df['Price'].isna().sum()\n",
    "        \n",
    "        if missing_before == 0:\n",
    "            print(f\"✓ No missing values found\")\n",
    "            return df\n",
    "        \n",
    "        print(f\"⚠ Missing values found: {missing_before} ({missing_before/len(df)*100:.2f}%)\")\n",
    "        \n",
    "        if method == 'forward_fill':\n",
    "            # Forward fill with backward fill for any remaining NaNs at the start\n",
    "            df['Price'] = df['Price'].fillna(method='ffill').fillna(method='bfill')\n",
    "            print(f\"✓ Applied forward fill (best for market data)\")\n",
    "        elif method == 'interpolate':\n",
    "            # Linear interpolation - good when prices jump unexpectedly\n",
    "            df['Price'] = df['Price'].interpolate(method='linear')\n",
    "            print(f\"✓ Applied linear interpolation\")\n",
    "        elif method == 'drop':\n",
    "            # Drop rows with missing prices\n",
    "            df = df.dropna(subset=['Price']).reset_index(drop=True)\n",
    "            print(f\"✓ Dropped {missing_before} rows with missing prices\")\n",
    "        \n",
    "        remaining_missing = df['Price'].isna().sum()\n",
    "        if remaining_missing > 0:\n",
    "            df = df.dropna(subset=['Price']).reset_index(drop=True)\n",
    "            print(f\"✓ Dropped remaining {remaining_missing} missing values\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def detect_outliers(self, df, window=20, std_threshold=3):\n",
    "        \"\"\"Detect and report potential outliers using rolling statistics\"\"\"\n",
    "        df = df.copy()\n",
    "        df['Returns'] = df['Price'].pct_change() * 100\n",
    "        \n",
    "        # Calculate rolling statistics\n",
    "        rolling_mean = df['Returns'].rolling(window=window).mean()\n",
    "        rolling_std = df['Returns'].rolling(window=window).std()\n",
    "        \n",
    "        # Identify outliers (beyond 3 standard deviations)\n",
    "        outlier_threshold_upper = rolling_mean + (std_threshold * rolling_std)\n",
    "        outlier_threshold_lower = rolling_mean - (std_threshold * rolling_std)\n",
    "        \n",
    "        outliers = (df['Returns'] > outlier_threshold_upper) | (df['Returns'] < outlier_threshold_lower)\n",
    "        num_outliers = outliers.sum()\n",
    "        \n",
    "        if num_outliers > 0:\n",
    "            print(f\"⚠ Detected {num_outliers} potential outliers ({num_outliers/len(df)*100:.2f}%)\")\n",
    "            outlier_dates = df[outliers]['Date'].dt.strftime('%Y-%m-%d').tolist()\n",
    "            print(f\"  Outlier dates: {', '.join(outlier_dates[:5])}\" + \n",
    "                  (f\" ... and {num_outliers-5} more\" if num_outliers > 5 else \"\"))\n",
    "        else:\n",
    "            print(f\"✓ No significant outliers detected\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def calculate_returns(self, prices):\n",
    "        \"\"\"Calculate logarithmic returns\"\"\"\n",
    "        returns = np.log(prices / prices.shift(1)).dropna()\n",
    "        return returns\n",
    "    \n",
    "    def create_sequences(self, data, lookback):\n",
    "        \"\"\"Create sequences for LSTM\"\"\"\n",
    "        X, y = [], []\n",
    "        for i in range(len(data) - lookback):\n",
    "            X.append(data[i:i + lookback])\n",
    "            y.append(data[i + lookback])\n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    def preprocess_for_deeplearning(self, df):\n",
    "        \"\"\"Preprocess data for deep learning models\"\"\"\n",
    "        prices = df['Price'].values.reshape(-1, 1)\n",
    "        returns = self.calculate_returns(df['Price']).values.reshape(-1, 1)\n",
    "        \n",
    "        # Normalize returns\n",
    "        returns_scaled = self.scaler.fit_transform(returns)\n",
    "        \n",
    "        # Create sequences\n",
    "        X, y = self.create_sequences(returns_scaled, self.lookback)\n",
    "        \n",
    "        return X, y, returns_scaled\n",
    "    \n",
    "    def split_train_test(self, X, y, test_size=0.2):\n",
    "        \"\"\"Split data into train and test sets\"\"\"\n",
    "        split_idx = int(len(X) * (1 - test_size))\n",
    "        X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "        y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    def get_data_summary(self, df, name=\"\"):\n",
    "        \"\"\"Get comprehensive data summary\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"DATA SUMMARY: {name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Date Range: {df['Date'].min().date()} to {df['Date'].max().date()}\")\n",
    "        print(f\"Total Records: {len(df)}\")\n",
    "        print(f\"Missing Values: {df['Price'].isna().sum()}\")\n",
    "        print(f\"\\nPrice Statistics:\")\n",
    "        print(f\"  Min:    ${df['Price'].min():.2f}\")\n",
    "        print(f\"  Max:    ${df['Price'].max():.2f}\")\n",
    "        print(f\"  Mean:   ${df['Price'].mean():.2f}\")\n",
    "        print(f\"  Median: ${df['Price'].median():.2f}\")\n",
    "        print(f\"  Std:    ${df['Price'].std():.2f}\")\n",
    "        \n",
    "        returns = self.calculate_returns(df['Price'])\n",
    "        print(f\"\\nReturn Statistics (Log Returns):\")\n",
    "        print(f\"  Mean:   {returns.mean()*100:.4f}%\")\n",
    "        print(f\"  Std:    {returns.std()*100:.4f}%\")\n",
    "        print(f\"  Min:    {returns.min()*100:.4f}%\")\n",
    "        print(f\"  Max:    {returns.max()*100:.4f}%\")\n",
    "        print(f\"  Skew:   {returns.skew():.4f}\")\n",
    "        print(f\"  Kurt:   {returns.kurtosis():.4f}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "\n",
    "print(\"✓ DataPreprocessor class created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "3f3e0d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ ModelEvaluator class created with VaR and confusion matrix!\n"
     ]
    }
   ],
   "source": [
    "# ==================== MODEL EVALUATOR ====================\n",
    "\n",
    "class ModelEvaluator:\n",
    "    \"\"\"Evaluate model performance with comprehensive metrics\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_metrics(y_true, y_pred):\n",
    "        \"\"\"Calculate evaluation metrics\"\"\"\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "        \n",
    "        return {'MAE': mae, 'RMSE': rmse, 'MAPE': mape}\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_var(returns, confidence_level=0.95):\n",
    "        \"\"\"Calculate VaR using historical simulation\"\"\"\n",
    "        var = np.percentile(returns, (1 - confidence_level) * 100)\n",
    "        return var\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_cvar(returns, confidence_level=0.95):\n",
    "        \"\"\"Calculate Conditional Value at Risk (Expected Shortfall)\"\"\"\n",
    "        var = np.percentile(returns, (1 - confidence_level) * 100)\n",
    "        cvar = returns[returns <= var].mean()\n",
    "        return cvar\n",
    "    \n",
    "    @staticmethod\n",
    "    def confusion_matrix_var(returns, predicted_var, confidence_level=0.95, threshold=None):\n",
    "        \"\"\"\n",
    "        Create confusion matrix for VaR predictions\n",
    "        Compares actual vs predicted risk events\n",
    "        \"\"\"\n",
    "        if threshold is None:\n",
    "            threshold = np.percentile(returns, (1 - confidence_level) * 100)\n",
    "        \n",
    "        # Actual events: returns below threshold\n",
    "        actual_events = (returns < threshold).astype(int)\n",
    "        # Predicted events: returns below predicted VaR\n",
    "        predicted_events = (returns < predicted_var).astype(int)\n",
    "        \n",
    "        # Confusion matrix\n",
    "        TP = np.sum((actual_events == 1) & (predicted_events == 1))\n",
    "        TN = np.sum((actual_events == 0) & (predicted_events == 0))\n",
    "        FP = np.sum((actual_events == 0) & (predicted_events == 1))\n",
    "        FN = np.sum((actual_events == 1) & (predicted_events == 0))\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = (TP + TN) / (TP + TN + FP + FN) if (TP + TN + FP + FN) > 0 else 0\n",
    "        precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "        recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'TP': TP, 'TN': TN, 'FP': FP, 'FN': FN,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_confusion_matrix(cm_dict, model_name, ax=None):\n",
    "        \"\"\"Plot confusion matrix for VaR predictions\"\"\"\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots(figsize=(6, 5))\n",
    "        \n",
    "        cm = np.array([[cm_dict['TN'], cm_dict['FP']], \n",
    "                       [cm_dict['FN'], cm_dict['TP']]])\n",
    "        \n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                    xticklabels=['No Risk', 'Risk'], \n",
    "                    yticklabels=['No Risk', 'Risk'],\n",
    "                    cbar=False, ax=ax, annot_kws={'size': 12, 'weight': 'bold'})\n",
    "        \n",
    "        ax.set_ylabel('Actual', fontsize=11, fontweight='bold')\n",
    "        ax.set_xlabel('Predicted', fontsize=11, fontweight='bold')\n",
    "        ax.set_title(f'{model_name} - VaR Confusion Matrix\\n(Confidence: 95%)', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "        \n",
    "        # Add metrics text\n",
    "        metrics_text = f\"Accuracy: {cm_dict['accuracy']:.3f}\\nPrecision: {cm_dict['precision']:.3f}\\n\"\n",
    "        metrics_text += f\"Recall: {cm_dict['recall']:.3f}\\nF1: {cm_dict['f1_score']:.3f}\"\n",
    "        ax.text(1.4, 0.5, metrics_text, transform=ax.transAxes, \n",
    "                fontsize=10, verticalalignment='center',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))\n",
    "\n",
    "print(\"✓ ModelEvaluator class created with VaR and confusion matrix!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "39ffa450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ ANNModel class created!\n"
     ]
    }
   ],
   "source": [
    "# ==================== ARTIFICIAL NEURAL NETWORK ====================\n",
    "\n",
    "class ANNModel:\n",
    "    \"\"\"Artificial Neural Network for return prediction\"\"\"\n",
    "    \n",
    "    def __init__(self, input_shape):\n",
    "        self.model = Sequential([\n",
    "            Dense(128, activation='relu', input_shape=(input_shape,)),\n",
    "            Dropout(0.2),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dropout(0.2),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dropout(0.1),\n",
    "            Dense(1, activation='linear')\n",
    "        ])\n",
    "        self.model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "        self.history = None\n",
    "        self.training_time = 0\n",
    "    \n",
    "    def train(self, X_train, y_train, epochs=50, batch_size=32, validation_split=0.1):\n",
    "        \"\"\"Train the ANN model\"\"\"\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        self.history = self.model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_split=validation_split,\n",
    "            callbacks=[early_stop],\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        self.training_time = time.time() - start_time\n",
    "        return self.history\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        return self.model.predict(X, verbose=0)\n",
    "    \n",
    "    def get_summary(self):\n",
    "        \"\"\"Get model summary\"\"\"\n",
    "        return self.model.summary()\n",
    "\n",
    "print(\"✓ ANNModel class created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d8e43617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ LSTMModel class created!\n"
     ]
    }
   ],
   "source": [
    "# ==================== LSTM NEURAL NETWORK ====================\n",
    "\n",
    "class LSTMModel:\n",
    "    \"\"\"Long Short-Term Memory Network for sequential prediction\"\"\"\n",
    "    \n",
    "    def __init__(self, input_shape):\n",
    "        self.model = Sequential([\n",
    "            LSTM(100, return_sequences=True, input_shape=(input_shape[0], input_shape[1])),\n",
    "            Dropout(0.2),\n",
    "            LSTM(50, return_sequences=False),\n",
    "            Dropout(0.2),\n",
    "            Dense(25, activation='relu'),\n",
    "            Dense(1, activation='linear')\n",
    "        ])\n",
    "        self.model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "        self.history = None\n",
    "        self.training_time = 0\n",
    "    \n",
    "    def train(self, X_train, y_train, epochs=50, batch_size=32, validation_split=0.1):\n",
    "        \"\"\"Train the LSTM model\"\"\"\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        self.history = self.model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_split=validation_split,\n",
    "            callbacks=[early_stop],\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        self.training_time = time.time() - start_time\n",
    "        return self.history\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        return self.model.predict(X, verbose=0)\n",
    "    \n",
    "    def get_summary(self):\n",
    "        \"\"\"Get model summary\"\"\"\n",
    "        return self.model.summary()\n",
    "\n",
    "print(\"✓ LSTMModel class created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "87b55f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CNNModel class created!\n"
     ]
    }
   ],
   "source": [
    "# ==================== CONVOLUTIONAL NEURAL NETWORK ====================\n",
    "\n",
    "class CNNModel:\n",
    "    \"\"\"Convolutional Neural Network adapted for time series\"\"\"\n",
    "    \n",
    "    def __init__(self, input_shape):\n",
    "        # Reshape input for CNN: (samples, timesteps, features, 1)\n",
    "        self.model = Sequential([\n",
    "            # First Conv Block\n",
    "            keras.layers.Conv1D(32, 3, padding='same', activation='relu', input_shape=(input_shape[0], input_shape[1])),\n",
    "            keras.layers.Conv1D(32, 3, activation='relu'),\n",
    "            keras.layers.MaxPooling1D(2),\n",
    "            Dropout(0.25),\n",
    "            \n",
    "            # Second Conv Block\n",
    "            keras.layers.Conv1D(64, 3, padding='same', activation='relu'),\n",
    "            keras.layers.Conv1D(64, 3, activation='relu'),\n",
    "            keras.layers.MaxPooling1D(2),\n",
    "            Dropout(0.25),\n",
    "            \n",
    "            # Third Conv Block\n",
    "            keras.layers.Conv1D(128, 3, padding='same', activation='relu'),\n",
    "            keras.layers.Conv1D(128, 3, activation='relu'),\n",
    "            keras.layers.MaxPooling1D(2),\n",
    "            Dropout(0.25),\n",
    "            \n",
    "            # Dense layers\n",
    "            keras.layers.Flatten(),\n",
    "            Dense(256, activation='relu'),\n",
    "            Dropout(0.5),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dropout(0.3),\n",
    "            Dense(1, activation='linear')\n",
    "        ])\n",
    "        \n",
    "        self.model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "        self.history = None\n",
    "        self.training_time = 0\n",
    "    \n",
    "    def train(self, X_train, y_train, epochs=50, batch_size=32, validation_split=0.1):\n",
    "        \"\"\"Train the CNN model\"\"\"\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        self.history = self.model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_split=validation_split,\n",
    "            callbacks=[early_stop],\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        self.training_time = time.time() - start_time\n",
    "        return self.history\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        return self.model.predict(X, verbose=0)\n",
    "    \n",
    "    def get_summary(self):\n",
    "        \"\"\"Get model summary\"\"\"\n",
    "        return self.model.summary()\n",
    "\n",
    "print(\"✓ CNNModel class created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "4f72276b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ ARIMAModel class created!\n"
     ]
    }
   ],
   "source": [
    "# ==================== ARIMA MODEL ====================\n",
    "\n",
    "class ARIMAModel:\n",
    "    \"\"\"ARIMA model for time series forecasting\"\"\"\n",
    "    \n",
    "    def __init__(self, order=(5, 1, 2)):\n",
    "        self.order = order\n",
    "        self.model = None\n",
    "        self.fitted_model = None\n",
    "        self.training_time = 0\n",
    "        self.predictions = []\n",
    "    \n",
    "    def fit(self, returns):\n",
    "        \"\"\"Fit ARIMA model\"\"\"\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        self.model = ARIMA(returns, order=self.order)\n",
    "        self.fitted_model = self.model.fit()\n",
    "        \n",
    "        self.training_time = time.time() - start_time\n",
    "        return self.fitted_model\n",
    "    \n",
    "    def predict(self, steps=1):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        forecast = self.fitted_model.get_forecast(steps=steps)\n",
    "        return forecast.predicted_mean.values\n",
    "    \n",
    "    def get_summary(self):\n",
    "        \"\"\"Get model summary\"\"\"\n",
    "        if self.fitted_model:\n",
    "            return self.fitted_model.summary()\n",
    "        return \"Model not fitted yet\"\n",
    "\n",
    "print(\"✓ ARIMAModel class created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0fd872e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ SARIMAModel class created!\n"
     ]
    }
   ],
   "source": [
    "# ==================== SARIMA MODEL ====================\n",
    "\n",
    "class SARIMAModel:\n",
    "    \"\"\"Seasonal ARIMA model for seasonal time series\"\"\"\n",
    "    \n",
    "    def __init__(self, order=(1, 1, 1), seasonal_order=(1, 1, 1, 12)):\n",
    "        self.order = order\n",
    "        self.seasonal_order = seasonal_order\n",
    "        self.model = None\n",
    "        self.fitted_model = None\n",
    "        self.training_time = 0\n",
    "        self.predictions = []\n",
    "    \n",
    "    def fit(self, returns):\n",
    "        \"\"\"Fit SARIMA model\"\"\"\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        self.model = SARIMAX(returns, order=self.order, seasonal_order=self.seasonal_order)\n",
    "        self.fitted_model = self.model.fit(disp=False)\n",
    "        \n",
    "        self.training_time = time.time() - start_time\n",
    "        return self.fitted_model\n",
    "    \n",
    "    def predict(self, steps=1):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        forecast = self.fitted_model.get_forecast(steps=steps)\n",
    "        return forecast.predicted_mean.values\n",
    "    \n",
    "    def get_summary(self):\n",
    "        \"\"\"Get model summary\"\"\"\n",
    "        if self.fitted_model:\n",
    "            return self.fitted_model.summary()\n",
    "        return \"Model not fitted yet\"\n",
    "\n",
    "print(\"✓ SARIMAModel class created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "d4dc548f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "VaR PREDICTION: DEEP LEARNING VS STATISTICAL MODELS\n",
      "MENA Region Stock Market Indices\n",
      "================================================================================\n",
      "\n",
      "Configuration:\n",
      "  Indices: ['Tunindex', 'ADI', 'MASI', 'TASI']\n",
      "  Lookback: 60\n",
      "  Epochs: 50\n"
     ]
    }
   ],
   "source": [
    "# ==================== MAIN EXECUTION SETUP ====================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"VaR PREDICTION: DEEP LEARNING VS STATISTICAL MODELS\")\n",
    "print(\"MENA Region Stock Market Indices\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Configuration\n",
    "indices = ['Tunindex', 'ADI', 'MASI', 'TASI']\n",
    "lookback = 60\n",
    "epochs = 50\n",
    "\n",
    "# Paths\n",
    "data_path = r'C:\\Users\\sfaxi\\Desktop\\Deep Learning\\data'\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Indices: {indices}\")\n",
    "print(f\"  Lookback: {lookback}\")\n",
    "print(f\"  Epochs: {epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "b7caeba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "LOADING & CLEANING DATA\n",
      "============================================================\n",
      "\n",
      "Processing ADI...\n",
      "  Loaded 2585 records\n",
      "✓ No missing values found\n",
      "⚠ Detected 13 potential outliers (0.50%)\n",
      "  Outlier dates: 2005-03-02, 2007-10-21, 2008-01-22, 2009-11-30, 2011-01-30 ... and 8 more\n",
      "\n",
      "============================================================\n",
      "DATA SUMMARY: ADI\n",
      "============================================================\n",
      "Date Range: 2005-01-03 to 2014-12-31\n",
      "Total Records: 2585\n",
      "Missing Values: 0\n",
      "\n",
      "Price Statistics:\n",
      "  Min:    $2136.64\n",
      "  Max:    $6237.98\n",
      "  Mean:   $3588.18\n",
      "  Median: $3298.11\n",
      "  Std:    $1031.20\n",
      "\n",
      "Return Statistics (Log Returns):\n",
      "  Mean:   0.0131%\n",
      "  Std:    1.2701%\n",
      "  Min:    -8.6793%\n",
      "  Max:    7.6295%\n",
      "  Skew:   -0.0614\n",
      "  Kurt:   6.9896\n",
      "============================================================\n",
      "\n",
      "\n",
      "Processing CAC40...\n",
      "  Loaded 2560 records\n",
      "✓ No missing values found\n",
      "⚠ Detected 9 potential outliers (0.35%)\n",
      "  Outlier dates: 2007-02-27, 2008-01-21, 2008-09-19, 2010-05-10, 2012-03-06 ... and 4 more\n",
      "\n",
      "============================================================\n",
      "DATA SUMMARY: CAC40\n",
      "============================================================\n",
      "Date Range: 2005-01-03 to 2014-12-31\n",
      "Total Records: 2560\n",
      "Missing Values: 0\n",
      "\n",
      "Price Statistics:\n",
      "  Min:    $2519.29\n",
      "  Max:    $6168.15\n",
      "  Mean:   $4174.61\n",
      "  Median: $4038.35\n",
      "  Std:    $797.36\n",
      "\n",
      "Return Statistics (Log Returns):\n",
      "  Mean:   0.0040%\n",
      "  Std:    1.4468%\n",
      "  Min:    -9.4715%\n",
      "  Max:    10.5946%\n",
      "  Skew:   0.0529\n",
      "  Kurt:   6.7784\n",
      "============================================================\n",
      "\n",
      "\n",
      "Processing MASI...\n",
      "  Loaded 2496 records\n",
      "✓ No missing values found\n",
      "⚠ Detected 17 potential outliers (0.68%)\n",
      "  Outlier dates: 2005-02-23, 2005-02-24, 2005-08-25, 2006-08-08, 2007-05-09 ... and 12 more\n",
      "\n",
      "============================================================\n",
      "DATA SUMMARY: MASI\n",
      "============================================================\n",
      "Date Range: 2005-01-03 to 2014-12-31\n",
      "Total Records: 2496\n",
      "Missing Values: 0\n",
      "\n",
      "Price Statistics:\n",
      "  Min:    $4352.67\n",
      "  Max:    $14925.99\n",
      "  Mean:   $10093.30\n",
      "  Median: $10393.35\n",
      "  Std:    $2475.51\n",
      "\n",
      "Return Statistics (Log Returns):\n",
      "  Mean:   0.0303%\n",
      "  Std:    0.8302%\n",
      "  Min:    -5.0167%\n",
      "  Max:    4.4635%\n",
      "  Skew:   -0.3904\n",
      "  Kurt:   4.8935\n",
      "============================================================\n",
      "\n",
      "\n",
      "Processing S&P500...\n",
      "  Loaded 2536 records\n",
      "✓ No missing values found\n",
      "⚠ Detected 5 potential outliers (0.20%)\n",
      "  Outlier dates: 2007-02-27, 2011-08-08, 2012-09-06, 2014-01-24, 2014-09-22\n",
      "\n",
      "============================================================\n",
      "DATA SUMMARY: S&P500\n",
      "============================================================\n",
      "Date Range: 2005-01-03 to 2014-12-31\n",
      "Total Records: 2536\n",
      "Missing Values: 0\n",
      "\n",
      "Price Statistics:\n",
      "  Min:    $676.00\n",
      "  Max:    $2085.75\n",
      "  Mean:   $1354.30\n",
      "  Median: $1314.50\n",
      "  Std:    $279.17\n",
      "\n",
      "Return Statistics (Log Returns):\n",
      "  Mean:   0.0210%\n",
      "  Std:    1.2974%\n",
      "  Min:    -10.4003%\n",
      "  Max:    13.2022%\n",
      "  Skew:   -0.1179\n",
      "  Kurt:   14.0981\n",
      "============================================================\n",
      "\n",
      "\n",
      "Processing TASI...\n",
      "  Loaded 2578 records\n",
      "✓ No missing values found\n",
      "⚠ Detected 14 potential outliers (0.54%)\n",
      "  Outlier dates: 2006-02-26, 2006-07-15, 2010-04-20, 2010-05-08, 2011-01-29 ... and 9 more\n",
      "\n",
      "============================================================\n",
      "DATA SUMMARY: TASI\n",
      "============================================================\n",
      "Date Range: 2005-01-03 to 2014-12-31\n",
      "Total Records: 2578\n",
      "Missing Values: 0\n",
      "\n",
      "Price Statistics:\n",
      "  Min:    $4130.01\n",
      "  Max:    $20634.86\n",
      "  Mean:   $8567.44\n",
      "  Median: $7593.16\n",
      "  Std:    $3011.33\n",
      "\n",
      "Return Statistics (Log Returns):\n",
      "  Mean:   0.0009%\n",
      "  Std:    1.6852%\n",
      "  Min:    -10.3285%\n",
      "  Max:    9.3907%\n",
      "  Skew:   -0.9023\n",
      "  Kurt:   8.2190\n",
      "============================================================\n",
      "\n",
      "\n",
      "Processing Tunindex...\n",
      "  Loaded 2471 records\n",
      "✓ No missing values found\n",
      "⚠ Detected 17 potential outliers (0.69%)\n",
      "  Outlier dates: 2005-04-06, 2005-05-31, 2008-03-31, 2008-08-11, 2008-10-06 ... and 12 more\n",
      "\n",
      "============================================================\n",
      "DATA SUMMARY: Tunindex\n",
      "============================================================\n",
      "Date Range: 2005-01-03 to 2014-12-31\n",
      "Total Records: 2471\n",
      "Missing Values: 0\n",
      "\n",
      "Price Statistics:\n",
      "  Min:    $1304.78\n",
      "  Max:    $5681.39\n",
      "  Mean:   $3623.76\n",
      "  Median: $4146.11\n",
      "  Std:    $1247.34\n",
      "\n",
      "Return Statistics (Log Returns):\n",
      "  Mean:   0.0542%\n",
      "  Std:    0.5868%\n",
      "  Min:    -5.0015%\n",
      "  Max:    4.1086%\n",
      "  Skew:   -0.5443\n",
      "  Kurt:   11.6434\n",
      "============================================================\n",
      "\n",
      "\n",
      "✓ Successfully loaded and cleaned 6 datasets\n"
     ]
    }
   ],
   "source": [
    "# ==================== DATA LOADING & INITIAL CLEANING ====================\n",
    "\n",
    "preprocessor = DataPreprocessor(lookback=60)\n",
    "\n",
    "datasets = {\n",
    "    'ADI': 'data/ADI.csv',\n",
    "    'CAC40': 'data/CAC40.csv',\n",
    "    'MASI': 'data/MASI.csv',\n",
    "    'S&P500': 'data/S&P500.csv',\n",
    "    'TASI': 'data/TASI.csv',\n",
    "    'Tunindex': 'data/Tunindex.csv'\n",
    "}\n",
    "\n",
    "all_data = {}\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LOADING & CLEANING DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for name, filepath in datasets.items():\n",
    "    try:\n",
    "        print(f\"\\nProcessing {name}...\")\n",
    "        df = preprocessor.load_data(filepath)\n",
    "        print(f\"  Loaded {len(df)} records\")\n",
    "        \n",
    "        df = preprocessor.handle_missing_values(df, method='forward_fill')\n",
    "        df = preprocessor.detect_outliers(df, window=20, std_threshold=3)\n",
    "        preprocessor.get_data_summary(df, name)\n",
    "        \n",
    "        all_data[name] = df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error processing {name}: {str(e)}\")\n",
    "\n",
    "print(f\"\\n✓ Successfully loaded and cleaned {len(all_data)} datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d2c739fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PROCESSING ALL INDICES\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Processing: Tunindex\n",
      "================================================================================\n",
      "\n",
      "Data shape: (2471, 7)\n",
      "Date range: 2005-01-03 00:00:00 to 2014-12-31 00:00:00\n",
      "\n",
      "Training set size: 1928\n",
      "Test set size: 482\n",
      "\n",
      "Training ANN Model...\n",
      "  Accuracy: 0.9379 | MAE: 0.03152391\n",
      "\n",
      "Training LSTM Model...\n",
      "  Accuracy: 0.9360 | MAE: 0.03207921\n",
      "\n",
      "Training CNN Model...\n",
      "  Accuracy: 0.9381 | MAE: 0.03156533\n",
      "\n",
      "Training ARIMA Model...\n",
      "  Accuracy: -184.4014 | MAE: 0.05017853\n",
      "\n",
      "Training SARIMA Model...\n",
      "  Accuracy: -184.8416 | MAE: 0.05029732\n",
      "\n",
      "\n",
      "================================================================================\n",
      "BEST MODEL FOR Tunindex: ANN\n",
      "================================================================================\n",
      "Accuracy: 0.9379 | MAE: 0.03152391\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Processing: ADI\n",
      "================================================================================\n",
      "\n",
      "Data shape: (2585, 7)\n",
      "Date range: 2005-01-03 00:00:00 to 2014-12-31 00:00:00\n",
      "\n",
      "Training set size: 2019\n",
      "Test set size: 505\n",
      "\n",
      "Training ANN Model...\n",
      "  Accuracy: 0.9004 | MAE: 0.04715889\n",
      "\n",
      "Training LSTM Model...\n",
      "  Accuracy: 0.9001 | MAE: 0.04790468\n",
      "\n",
      "Training CNN Model...\n",
      "  Accuracy: 0.9005 | MAE: 0.04790593\n",
      "\n",
      "Training ARIMA Model...\n",
      "  Accuracy: -222.2122 | MAE: 0.08694276\n",
      "\n",
      "Training SARIMA Model...\n",
      "  Accuracy: -223.7510 | MAE: 0.08753517\n",
      "\n",
      "\n",
      "================================================================================\n",
      "BEST MODEL FOR ADI: ANN\n",
      "================================================================================\n",
      "Accuracy: 0.9004 | MAE: 0.04715889\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Processing: MASI\n",
      "================================================================================\n",
      "\n",
      "Data shape: (2496, 7)\n",
      "Date range: 2005-01-03 00:00:00 to 2014-12-31 00:00:00\n",
      "\n",
      "Training set size: 1948\n",
      "Test set size: 487\n",
      "\n",
      "Training ANN Model...\n",
      "  Accuracy: 0.9224 | MAE: 0.04074503\n",
      "\n",
      "Training LSTM Model...\n",
      "  Accuracy: 0.9223 | MAE: 0.04072096\n",
      "\n",
      "Training CNN Model...\n",
      "  Accuracy: 0.9232 | MAE: 0.04022718\n",
      "\n",
      "Training ARIMA Model...\n",
      "  Accuracy: -88.8170 | MAE: 0.05035559\n",
      "\n",
      "Training SARIMA Model...\n",
      "  Accuracy: -88.6717 | MAE: 0.05027424\n",
      "\n",
      "\n",
      "================================================================================\n",
      "BEST MODEL FOR MASI: CNN\n",
      "================================================================================\n",
      "Accuracy: 0.9232 | MAE: 0.04022718\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Processing: TASI\n",
      "================================================================================\n",
      "\n",
      "Data shape: (2578, 7)\n",
      "Date range: 2005-01-03 00:00:00 to 2014-12-31 00:00:00\n",
      "\n",
      "Training set size: 2013\n",
      "Test set size: 504\n",
      "\n",
      "Training ANN Model...\n",
      "  Accuracy: 0.9342 | MAE: 0.02988554\n",
      "\n",
      "Training LSTM Model...\n",
      "  Accuracy: 0.9337 | MAE: 0.02986710\n",
      "\n",
      "Training CNN Model...\n",
      "  Accuracy: 0.9348 | MAE: 0.02938790\n",
      "\n",
      "Training ARIMA Model...\n",
      "  Accuracy: -198.6117 | MAE: 0.10358997\n",
      "\n",
      "Training SARIMA Model...\n",
      "  Accuracy: -200.2936 | MAE: 0.10446040\n",
      "\n",
      "\n",
      "================================================================================\n",
      "BEST MODEL FOR TASI: CNN\n",
      "================================================================================\n",
      "Accuracy: 0.9348 | MAE: 0.02938790\n",
      "\n",
      "\n",
      "================================================================================\n",
      "✓ All indices processed successfully\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==================== PROCESS ALL INDICES ====================\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(f\"PROCESSING ALL INDICES\")\n",
    "print(f\"{'=' * 80}\\n\")\n",
    "\n",
    "# Loop through all indices\n",
    "for index in indices:\n",
    "    try:\n",
    "        print(f\"\\n{'=' * 80}\")\n",
    "        print(f\"Processing: {index}\")\n",
    "        print(f\"{'=' * 80}\\n\")\n",
    "        \n",
    "        preprocessor = DataPreprocessor(lookback=lookback)\n",
    "        df = preprocessor.load_data(rf\"{data_path}\\{index}.csv\")\n",
    "        \n",
    "        print(f\"Data shape: {df.shape}\")\n",
    "        print(f\"Date range: {df['Date'].min()} to {df['Date'].max()}\\n\")\n",
    "        \n",
    "        X, y, returns_scaled = preprocessor.preprocess_for_deeplearning(df)\n",
    "        X_train, X_test, y_train, y_test = preprocessor.split_train_test(X, y)\n",
    "        \n",
    "        print(f\"Training set size: {X_train.shape[0]}\")\n",
    "        print(f\"Test set size: {X_test.shape[0]}\\n\")\n",
    "        \n",
    "        # ==================== TRAIN ANN MODEL ====================\n",
    "        print(\"Training ANN Model...\")\n",
    "        ann_model = ANNModel(input_shape=X_train.shape[1] * X_train.shape[2])\n",
    "        X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "        X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "        \n",
    "        history_ann = ann_model.train(X_train_flat, y_train, epochs=epochs)\n",
    "        y_pred_ann = ann_model.predict(X_test_flat).flatten()\n",
    "        metrics_ann = ModelEvaluator.calculate_metrics(y_test, y_pred_ann)\n",
    "        \n",
    "        print(f\"  Accuracy: {(1 - metrics_ann['MAPE']/100):.4f} | MAE: {metrics_ann['MAE']:.8f}\\n\")\n",
    "        \n",
    "        # ==================== TRAIN LSTM MODEL ====================\n",
    "        print(\"Training LSTM Model...\")\n",
    "        lstm_model = LSTMModel(input_shape=(X_train.shape[1], X_train.shape[2]))\n",
    "        history_lstm = lstm_model.train(X_train, y_train, epochs=epochs)\n",
    "        y_pred_lstm = lstm_model.predict(X_test).flatten()\n",
    "        metrics_lstm = ModelEvaluator.calculate_metrics(y_test, y_pred_lstm)\n",
    "        \n",
    "        print(f\"  Accuracy: {(1 - metrics_lstm['MAPE']/100):.4f} | MAE: {metrics_lstm['MAE']:.8f}\\n\")\n",
    "        \n",
    "        # ==================== TRAIN CNN MODEL ====================\n",
    "        print(\"Training CNN Model...\")\n",
    "        cnn_model = CNNModel(input_shape=(X_train.shape[1], X_train.shape[2]))\n",
    "        history_cnn = cnn_model.train(X_train, y_train, epochs=epochs)\n",
    "        y_pred_cnn = cnn_model.predict(X_test).flatten()\n",
    "        metrics_cnn = ModelEvaluator.calculate_metrics(y_test, y_pred_cnn)\n",
    "        \n",
    "        print(f\"  Accuracy: {(1 - metrics_cnn['MAPE']/100):.4f} | MAE: {metrics_cnn['MAE']:.8f}\\n\")\n",
    "        \n",
    "        # ==================== TRAIN ARIMA MODEL ====================\n",
    "        print(\"Training ARIMA Model...\")\n",
    "        returns = preprocessor.calculate_returns(df['Price'])\n",
    "        arima_model = ARIMAModel(order=(5, 1, 2))\n",
    "        arima_model.fit(returns)\n",
    "        \n",
    "        y_pred_arima = np.array([arima_model.predict(steps=1)[0] for _ in range(len(y_test))])\n",
    "        y_pred_arima_denorm = preprocessor.scaler.inverse_transform(y_pred_arima.reshape(-1, 1)).flatten()\n",
    "        y_test_denorm = preprocessor.scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "        metrics_arima = ModelEvaluator.calculate_metrics(y_test_denorm, y_pred_arima_denorm)\n",
    "        \n",
    "        print(f\"  Accuracy: {(1 - metrics_arima['MAPE']/100):.4f} | MAE: {metrics_arima['MAE']:.8f}\\n\")\n",
    "        \n",
    "        # ==================== TRAIN SARIMA MODEL ====================\n",
    "        print(\"Training SARIMA Model...\")\n",
    "        try:\n",
    "            sarima_model = SARIMAModel(order=(1, 1, 1), seasonal_order=(1, 1, 1, 12))\n",
    "            sarima_model.fit(returns)\n",
    "            \n",
    "            y_pred_sarima = np.array([sarima_model.predict(steps=1)[0] for _ in range(len(y_test))])\n",
    "            y_pred_sarima_denorm = preprocessor.scaler.inverse_transform(y_pred_sarima.reshape(-1, 1)).flatten()\n",
    "            metrics_sarima = ModelEvaluator.calculate_metrics(y_test_denorm, y_pred_sarima_denorm)\n",
    "            \n",
    "            print(f\"  Accuracy: {(1 - metrics_sarima['MAPE']/100):.4f} | MAE: {metrics_sarima['MAE']:.8f}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ⚠ SARIMA failed: {str(e)}\\n\")\n",
    "            metrics_sarima = metrics_arima.copy()\n",
    "        \n",
    "        # ==================== CALCULATE VaR ====================\n",
    "        returns_denorm = preprocessor.scaler.inverse_transform(returns.values.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        n_bootstrap = 1000\n",
    "        var_95_ann_bs = [np.percentile(np.random.choice(y_pred_ann, len(y_pred_ann), replace=True), 5) for _ in range(n_bootstrap)]\n",
    "        var_99_ann_bs = [np.percentile(np.random.choice(y_pred_ann, len(y_pred_ann), replace=True), 1) for _ in range(n_bootstrap)]\n",
    "        var_95_lstm_bs = [np.percentile(np.random.choice(y_pred_lstm, len(y_pred_lstm), replace=True), 5) for _ in range(n_bootstrap)]\n",
    "        var_99_lstm_bs = [np.percentile(np.random.choice(y_pred_lstm, len(y_pred_lstm), replace=True), 1) for _ in range(n_bootstrap)]\n",
    "        var_95_cnn_bs = [np.percentile(np.random.choice(y_pred_cnn, len(y_pred_cnn), replace=True), 5) for _ in range(n_bootstrap)]\n",
    "        var_99_cnn_bs = [np.percentile(np.random.choice(y_pred_cnn, len(y_pred_cnn), replace=True), 1) for _ in range(n_bootstrap)]\n",
    "        var_95_arima_bs = [np.percentile(np.random.choice(y_pred_arima_denorm, len(y_pred_arima_denorm), replace=True), 5) for _ in range(n_bootstrap)]\n",
    "        var_99_arima_bs = [np.percentile(np.random.choice(y_pred_arima_denorm, len(y_pred_arima_denorm), replace=True), 1) for _ in range(n_bootstrap)]\n",
    "        \n",
    "        var_95_ann = np.mean(var_95_ann_bs)\n",
    "        var_99_ann = np.mean(var_99_ann_bs)\n",
    "        var_95_lstm = np.mean(var_95_lstm_bs)\n",
    "        var_99_lstm = np.mean(var_99_lstm_bs)\n",
    "        var_95_cnn = np.mean(var_95_cnn_bs)\n",
    "        var_99_cnn = np.mean(var_99_cnn_bs)\n",
    "        var_95_arima = np.mean(var_95_arima_bs)\n",
    "        var_99_arima = np.mean(var_99_arima_bs)\n",
    "        \n",
    "        # Create and store results\n",
    "        performance_summary = {\n",
    "            'Model': ['ANN', 'LSTM', 'CNN', 'ARIMA', 'SARIMA'],\n",
    "            'Accuracy': [\n",
    "                1 - metrics_ann['MAPE']/100,\n",
    "                1 - metrics_lstm['MAPE']/100,\n",
    "                1 - metrics_cnn['MAPE']/100,\n",
    "                1 - metrics_arima['MAPE']/100,\n",
    "                1 - metrics_sarima['MAPE']/100\n",
    "            ],\n",
    "            'MAE': [metrics_ann['MAE'], metrics_lstm['MAE'], metrics_cnn['MAE'], metrics_arima['MAE'], metrics_sarima['MAE']],\n",
    "            'RMSE': [metrics_ann['RMSE'], metrics_lstm['RMSE'], metrics_cnn['RMSE'], metrics_arima['RMSE'], metrics_sarima['RMSE']],\n",
    "            'MAPE (%)': [metrics_ann['MAPE'], metrics_lstm['MAPE'], metrics_cnn['MAPE'], metrics_arima['MAPE'], metrics_sarima['MAPE']],\n",
    "            'VaR 95%': [var_95_ann, var_95_lstm, var_95_cnn, var_95_arima, var_95_arima],\n",
    "            'VaR 99%': [var_99_ann, var_99_lstm, var_99_cnn, var_99_arima, var_99_arima],\n",
    "            'Type': ['Deep Learning', 'Deep Learning', 'Deep Learning', 'Statistical', 'Statistical']\n",
    "        }\n",
    "        \n",
    "        perf_df = pd.DataFrame(performance_summary)\n",
    "        best_model_idx = perf_df['MAE'].idxmin()\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"BEST MODEL FOR {index}: {perf_df.loc[best_model_idx, 'Model']}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Accuracy: {perf_df.loc[best_model_idx, 'Accuracy']:.4f} | MAE: {perf_df.loc[best_model_idx, 'MAE']:.8f}\\n\")\n",
    "        \n",
    "        # Store results\n",
    "        results[index] = perf_df.to_dict('list')\n",
    "        results[index]['Index'] = index\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error processing {index}: {str(e)}\\n\")\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(f\"✓ All indices processed successfully\")\n",
    "print(f\"{'=' * 80}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "ecd2ac84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================================================================================================\n",
      "FINAL RESULTS - ALL INDICES & BEST MODELS\n",
      "================================================================================================================================================================\n",
      "\n",
      "Index           Best Model      Accuracy     MAE             VaR 95%         VaR 99%        \n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Tunindex        ANN             0.9379       0.03152391      0.54492307      0.54243565     \n",
      "ADI             ANN             0.9004       0.04715889      0.52467543      0.52062058     \n",
      "MASI            CNN             0.9232       0.04022718      0.52172631      0.52171826     \n",
      "TASI            CNN             0.9348       0.02938790      0.52349389      0.52348596     \n",
      "\n",
      "================================================================================================================================================================\n",
      "KEY FINDINGS\n",
      "\n",
      "✓ Best Overall: TASI - CNN\n",
      "  Accuracy: 0.9348\n",
      "\n",
      "✓ Best Model Per Index:\n",
      "  • Tunindex: ANN (Accuracy: 0.9379)\n",
      "  • ADI: ANN (Accuracy: 0.9004)\n",
      "  • MASI: CNN (Accuracy: 0.9232)\n",
      "  • TASI: CNN (Accuracy: 0.9348)\n",
      "\n",
      "================================================================================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==================== FINAL RESULTS TABLE - ALL INDICES ====================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 160)\n",
    "print(\"FINAL RESULTS - ALL INDICES & BEST MODELS\")\n",
    "print(\"=\" * 160 + \"\\n\")\n",
    "\n",
    "# Create summary table for all indices\n",
    "summary_data = []\n",
    "\n",
    "for idx, result in results.items():\n",
    "    mae_values = result['MAE']\n",
    "    best_idx = mae_values.index(min(mae_values))\n",
    "    best_model = result['Model'][best_idx]\n",
    "    best_accuracy = result['Accuracy'][best_idx]\n",
    "    best_mae = result['MAE'][best_idx]\n",
    "    best_var_95 = result['VaR 95%'][best_idx]\n",
    "    best_var_99 = result['VaR 99%'][best_idx]\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Index': idx,\n",
    "        'Best Model': best_model,\n",
    "        'Accuracy': best_accuracy,\n",
    "        'MAE': best_mae,\n",
    "        'VaR 95%': best_var_95,\n",
    "        'VaR 99%': best_var_99\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "print(f\"{'Index':<15} {'Best Model':<15} {'Accuracy':<12} {'MAE':<15} {'VaR 95%':<15} {'VaR 99%':<15}\")\n",
    "print(\"─\" * 160)\n",
    "\n",
    "for _, row in summary_df.iterrows():\n",
    "    print(f\"{row['Index']:<15} {row['Best Model']:<15} {row['Accuracy']:<12.4f} {row['MAE']:<15.8f} {row['VaR 95%']:<15.8f} {row['VaR 99%']:<15.8f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 160)\n",
    "print(\"KEY FINDINGS\\n\")\n",
    "\n",
    "overall_best_idx = summary_df['MAE'].idxmin()\n",
    "print(f\"✓ Best Overall: {summary_df.loc[overall_best_idx, 'Index']} - {summary_df.loc[overall_best_idx, 'Best Model']}\")\n",
    "print(f\"  Accuracy: {summary_df.loc[overall_best_idx, 'Accuracy']:.4f}\\n\")\n",
    "\n",
    "print(\"✓ Best Model Per Index:\")\n",
    "for _, row in summary_df.iterrows():\n",
    "    print(f\"  • {row['Index']}: {row['Best Model']} (Accuracy: {row['Accuracy']:.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 160 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
