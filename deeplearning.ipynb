{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c35c41f4",
   "metadata": {},
   "source": [
    "# Deep Learning vs Statistical Models for VaR Prediction\n",
    "## MENA Region Stock Market Analysis\n",
    "\n",
    "This notebook implements and compares deep learning models (ANN, LSTM, CNN) with statistical models (ARIMA, SARIMA) for predicting Value-at-Risk in MENA stock indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f001d913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# TensorFlow/Keras imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Statsmodels for ARIMA/SARIMA\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c03f36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ DataPreprocessor class created!\n"
     ]
    }
   ],
   "source": [
    "# ==================== DATA LOADING & PREPROCESSING ====================\n",
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(self, lookback=60):\n",
    "        self.lookback = lookback\n",
    "        self.scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        self.data_stats = {}\n",
    "    \n",
    "    def load_data(self, filepath):\n",
    "        \"\"\"Load CSV data and handle formatting\"\"\"\n",
    "        df = pd.read_csv(filepath)\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        df = df.sort_values('Date').reset_index(drop=True)\n",
    "        df['Price'] = pd.to_numeric(df['Price'].str.replace(',', ''), errors='coerce')\n",
    "        return df\n",
    "    \n",
    "    def handle_missing_values(self, df, method='forward_fill'):\n",
    "        \"\"\"\n",
    "        Methods:\n",
    "        - 'forward_fill': Use last known price (default, best for market holidays/weekends)\n",
    "        - 'interpolate': Linear interpolation between values\n",
    "        - 'drop': Remove rows with missing values\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        missing_before = df['Price'].isna().sum()\n",
    "        \n",
    "        if missing_before == 0:\n",
    "            print(f\"âœ“ No missing values found\")\n",
    "            return df\n",
    "        \n",
    "        print(f\"âš  Missing values found: {missing_before} ({missing_before/len(df)*100:.2f}%)\")\n",
    "        \n",
    "        if method == 'forward_fill':\n",
    "            # Forward fill with backward fill for any remaining NaNs at the start\n",
    "            df['Price'] = df['Price'].fillna(method='ffill').fillna(method='bfill')\n",
    "            print(f\"âœ“ Applied forward fill (best for market data)\")\n",
    "        elif method == 'interpolate':\n",
    "            # Linear interpolation - good when prices jump unexpectedly\n",
    "            df['Price'] = df['Price'].interpolate(method='linear')\n",
    "            print(f\"âœ“ Applied linear interpolation\")\n",
    "        elif method == 'drop':\n",
    "            # Drop rows with missing prices\n",
    "            df = df.dropna(subset=['Price']).reset_index(drop=True)\n",
    "            print(f\"âœ“ Dropped {missing_before} rows with missing prices\")\n",
    "        \n",
    "        remaining_missing = df['Price'].isna().sum()\n",
    "        if remaining_missing > 0:\n",
    "            df = df.dropna(subset=['Price']).reset_index(drop=True)\n",
    "            print(f\"âœ“ Dropped remaining {remaining_missing} missing values\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def detect_outliers(self, df, window=20, std_threshold=3):\n",
    "        \"\"\"Detect and report potential outliers using rolling statistics\"\"\"\n",
    "        df = df.copy()\n",
    "        df['Returns'] = df['Price'].pct_change() * 100\n",
    "        \n",
    "        # Calculate rolling statistics\n",
    "        rolling_mean = df['Returns'].rolling(window=window).mean()\n",
    "        rolling_std = df['Returns'].rolling(window=window).std()\n",
    "        \n",
    "        # Identify outliers (beyond 3 standard deviations)\n",
    "        outlier_threshold_upper = rolling_mean + (std_threshold * rolling_std)\n",
    "        outlier_threshold_lower = rolling_mean - (std_threshold * rolling_std)\n",
    "        \n",
    "        outliers = (df['Returns'] > outlier_threshold_upper) | (df['Returns'] < outlier_threshold_lower)\n",
    "        num_outliers = outliers.sum()\n",
    "        \n",
    "        if num_outliers > 0:\n",
    "            print(f\"âš  Detected {num_outliers} potential outliers ({num_outliers/len(df)*100:.2f}%)\")\n",
    "            outlier_dates = df[outliers]['Date'].dt.strftime('%Y-%m-%d').tolist()\n",
    "            print(f\"  Outlier dates: {', '.join(outlier_dates[:5])}\" + \n",
    "                  (f\" ... and {num_outliers-5} more\" if num_outliers > 5 else \"\"))\n",
    "        else:\n",
    "            print(f\"âœ“ No significant outliers detected\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def calculate_returns(self, prices):\n",
    "        \"\"\"Calculate logarithmic returns\"\"\"\n",
    "        returns = np.log(prices / prices.shift(1)).dropna()\n",
    "        return returns\n",
    "    \n",
    "    def create_sequences(self, data, lookback):\n",
    "        \"\"\"Create sequences for LSTM\"\"\"\n",
    "        X, y = [], []\n",
    "        for i in range(len(data) - lookback):\n",
    "            X.append(data[i:i + lookback])\n",
    "            y.append(data[i + lookback])\n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    def preprocess_for_deeplearning(self, df):\n",
    "        \"\"\"Preprocess data for deep learning models\"\"\"\n",
    "        prices = df['Price'].values.reshape(-1, 1)\n",
    "        returns = self.calculate_returns(df['Price']).values.reshape(-1, 1)\n",
    "        \n",
    "        # Normalize returns\n",
    "        returns_scaled = self.scaler.fit_transform(returns)\n",
    "        \n",
    "        # Create sequences\n",
    "        X, y = self.create_sequences(returns_scaled, self.lookback)\n",
    "        \n",
    "        return X, y, returns_scaled\n",
    "    \n",
    "    def split_train_test(self, X, y, test_size=0.2):\n",
    "        \"\"\"Split data into train and test sets\"\"\"\n",
    "        split_idx = int(len(X) * (1 - test_size))\n",
    "        X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "        y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    def get_data_summary(self, df, name=\"\"):\n",
    "        \"\"\"Get comprehensive data summary\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"DATA SUMMARY: {name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Date Range: {df['Date'].min().date()} to {df['Date'].max().date()}\")\n",
    "        print(f\"Total Records: {len(df)}\")\n",
    "        print(f\"Missing Values: {df['Price'].isna().sum()}\")\n",
    "        print(f\"\\nPrice Statistics:\")\n",
    "        print(f\"  Min:    ${df['Price'].min():.2f}\")\n",
    "        print(f\"  Max:    ${df['Price'].max():.2f}\")\n",
    "        print(f\"  Mean:   ${df['Price'].mean():.2f}\")\n",
    "        print(f\"  Median: ${df['Price'].median():.2f}\")\n",
    "        print(f\"  Std:    ${df['Price'].std():.2f}\")\n",
    "        \n",
    "        returns = self.calculate_returns(df['Price'])\n",
    "        print(f\"\\nReturn Statistics (Log Returns):\")\n",
    "        print(f\"  Mean:   {returns.mean()*100:.4f}%\")\n",
    "        print(f\"  Std:    {returns.std()*100:.4f}%\")\n",
    "        print(f\"  Min:    {returns.min()*100:.4f}%\")\n",
    "        print(f\"  Max:    {returns.max()*100:.4f}%\")\n",
    "        print(f\"  Skew:   {returns.skew():.4f}\")\n",
    "        print(f\"  Kurt:   {returns.kurtosis():.4f}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "\n",
    "print(\"âœ“ DataPreprocessor class created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3e0d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ ModelEvaluator class created!\n"
     ]
    }
   ],
   "source": [
    "# ==================== MODEL EVALUATOR ====================\n",
    "\n",
    "class ModelEvaluator:\n",
    "    \"\"\"Evaluate model performance with comprehensive metrics\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_metrics(y_true, y_pred):\n",
    "        \"\"\"Calculate evaluation metrics\"\"\"\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "        \n",
    "        return {'MAE': mae, 'RMSE': rmse, 'MAPE': mape}\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_var(returns, confidence_level=0.95):\n",
    "        \"\"\"Calculate VaR using historical simulation\"\"\"\n",
    "        var = np.percentile(returns, (1 - confidence_level) * 100)\n",
    "        return var\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_cvar(returns, confidence_level=0.95):\n",
    "        \"\"\"Calculate Conditional Value at Risk (Expected Shortfall)\"\"\"\n",
    "        var = np.percentile(returns, (1 - confidence_level) * 100)\n",
    "        cvar = returns[returns <= var].mean()\n",
    "        return cvar\n",
    "    \n",
    "    @staticmethod\n",
    "    def confusion_matrix_var(returns, predicted_var, confidence_level=0.95, threshold=None):\n",
    "        \"\"\"\n",
    "        Create confusion matrix for VaR predictions\n",
    "        Compares actual vs predicted risk events\n",
    "        \"\"\"\n",
    "        if threshold is None:\n",
    "            threshold = np.percentile(returns, (1 - confidence_level) * 100)\n",
    "        \n",
    "        # Actual events: returns below threshold\n",
    "        actual_events = (returns < threshold).astype(int)\n",
    "        # Predicted events: returns below predicted VaR\n",
    "        predicted_events = (returns < predicted_var).astype(int)\n",
    "        \n",
    "        # Confusion matrix\n",
    "        TP = np.sum((actual_events == 1) & (predicted_events == 1))\n",
    "        TN = np.sum((actual_events == 0) & (predicted_events == 0))\n",
    "        FP = np.sum((actual_events == 0) & (predicted_events == 1))\n",
    "        FN = np.sum((actual_events == 1) & (predicted_events == 0))\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = (TP + TN) / (TP + TN + FP + FN) if (TP + TN + FP + FN) > 0 else 0\n",
    "        precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "        recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'TP': TP, 'TN': TN, 'FP': FP, 'FN': FN,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_confusion_matrix(cm_dict, model_name, ax=None):\n",
    "        \"\"\"Plot confusion matrix for VaR predictions\"\"\"\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots(figsize=(6, 5))\n",
    "        \n",
    "        cm = np.array([[cm_dict['TN'], cm_dict['FP']], \n",
    "                       [cm_dict['FN'], cm_dict['TP']]])\n",
    "        \n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                    xticklabels=['No Risk', 'Risk'], \n",
    "                    yticklabels=['No Risk', 'Risk'],\n",
    "                    cbar=False, ax=ax, annot_kws={'size': 12, 'weight': 'bold'})\n",
    "        \n",
    "        ax.set_ylabel('Actual', fontsize=11, fontweight='bold')\n",
    "        ax.set_xlabel('Predicted', fontsize=11, fontweight='bold')\n",
    "        ax.set_title(f'{model_name} - VaR Confusion Matrix\\n(Confidence: 95%)', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "        \n",
    "        # Add metrics text\n",
    "        metrics_text = f\"Accuracy: {cm_dict['accuracy']:.3f}\\nPrecision: {cm_dict['precision']:.3f}\\n\"\n",
    "        metrics_text += f\"Recall: {cm_dict['recall']:.3f}\\nF1: {cm_dict['f1_score']:.3f}\"\n",
    "        ax.text(1.4, 0.5, metrics_text, transform=ax.transAxes, \n",
    "                fontsize=10, verticalalignment='center',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))\n",
    "\n",
    "print(\"âœ“ ModelEvaluator class created with VaR and confusion matrix!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39ffa450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ ANNModel class created!\n"
     ]
    }
   ],
   "source": [
    "# ==================== ARTIFICIAL NEURAL NETWORK ====================\n",
    "\n",
    "class ANNModel:\n",
    "    \"\"\"Artificial Neural Network for return prediction\"\"\"\n",
    "    \n",
    "    def __init__(self, input_shape):\n",
    "        self.model = Sequential([\n",
    "            Dense(128, activation='relu', input_shape=(input_shape,)),\n",
    "            Dropout(0.2),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dropout(0.2),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dropout(0.1),\n",
    "            Dense(1, activation='linear')\n",
    "        ])\n",
    "        self.model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "        self.history = None\n",
    "        self.training_time = 0\n",
    "    \n",
    "    def train(self, X_train, y_train, epochs=50, batch_size=32, validation_split=0.1):\n",
    "        \"\"\"Train the ANN model\"\"\"\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        self.history = self.model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_split=validation_split,\n",
    "            callbacks=[early_stop],\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        self.training_time = time.time() - start_time\n",
    "        return self.history\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        return self.model.predict(X, verbose=0)\n",
    "    \n",
    "    def get_summary(self):\n",
    "        \"\"\"Get model summary\"\"\"\n",
    "        return self.model.summary()\n",
    "\n",
    "print(\"âœ“ ANNModel class created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8e43617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ LSTMModel class created!\n"
     ]
    }
   ],
   "source": [
    "# ==================== LSTM NEURAL NETWORK ====================\n",
    "\n",
    "class LSTMModel:\n",
    "    \"\"\"Long Short-Term Memory Network for sequential prediction\"\"\"\n",
    "    \n",
    "    def __init__(self, input_shape):\n",
    "        self.model = Sequential([\n",
    "            LSTM(100, return_sequences=True, input_shape=(input_shape[0], input_shape[1])),\n",
    "            Dropout(0.2),\n",
    "            LSTM(50, return_sequences=False),\n",
    "            Dropout(0.2),\n",
    "            Dense(25, activation='relu'),\n",
    "            Dense(1, activation='linear')\n",
    "        ])\n",
    "        self.model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "        self.history = None\n",
    "        self.training_time = 0\n",
    "    \n",
    "    def train(self, X_train, y_train, epochs=50, batch_size=32, validation_split=0.1):\n",
    "        \"\"\"Train the LSTM model\"\"\"\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        self.history = self.model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_split=validation_split,\n",
    "            callbacks=[early_stop],\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        self.training_time = time.time() - start_time\n",
    "        return self.history\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        return self.model.predict(X, verbose=0)\n",
    "    \n",
    "    def get_summary(self):\n",
    "        \"\"\"Get model summary\"\"\"\n",
    "        return self.model.summary()\n",
    "\n",
    "print(\"âœ“ LSTMModel class created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87b55f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ CNNModel class created!\n"
     ]
    }
   ],
   "source": [
    "# ==================== CONVOLUTIONAL NEURAL NETWORK ====================\n",
    "\n",
    "class CNNModel:\n",
    "    \"\"\"Convolutional Neural Network adapted for time series\"\"\"\n",
    "    \n",
    "    def __init__(self, input_shape):\n",
    "        # Reshape input for CNN: (samples, timesteps, features, 1)\n",
    "        self.model = Sequential([\n",
    "            # First Conv Block\n",
    "            keras.layers.Conv1D(32, 3, padding='same', activation='relu', input_shape=(input_shape[0], input_shape[1])),\n",
    "            keras.layers.Conv1D(32, 3, activation='relu'),\n",
    "            keras.layers.MaxPooling1D(2),\n",
    "            Dropout(0.25),\n",
    "            \n",
    "            # Second Conv Block\n",
    "            keras.layers.Conv1D(64, 3, padding='same', activation='relu'),\n",
    "            keras.layers.Conv1D(64, 3, activation='relu'),\n",
    "            keras.layers.MaxPooling1D(2),\n",
    "            Dropout(0.25),\n",
    "            \n",
    "            # Third Conv Block\n",
    "            keras.layers.Conv1D(128, 3, padding='same', activation='relu'),\n",
    "            keras.layers.Conv1D(128, 3, activation='relu'),\n",
    "            keras.layers.MaxPooling1D(2),\n",
    "            Dropout(0.25),\n",
    "            \n",
    "            # Dense layers\n",
    "            keras.layers.Flatten(),\n",
    "            Dense(256, activation='relu'),\n",
    "            Dropout(0.5),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dropout(0.3),\n",
    "            Dense(1, activation='linear')\n",
    "        ])\n",
    "        \n",
    "        self.model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "        self.history = None\n",
    "        self.training_time = 0\n",
    "    \n",
    "    def train(self, X_train, y_train, epochs=50, batch_size=32, validation_split=0.1):\n",
    "        \"\"\"Train the CNN model\"\"\"\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        self.history = self.model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_split=validation_split,\n",
    "            callbacks=[early_stop],\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        self.training_time = time.time() - start_time\n",
    "        return self.history\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        return self.model.predict(X, verbose=0)\n",
    "    \n",
    "    def get_summary(self):\n",
    "        \"\"\"Get model summary\"\"\"\n",
    "        return self.model.summary()\n",
    "\n",
    "print(\"âœ“ CNNModel class created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f72276b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ ARIMAModel class created!\n"
     ]
    }
   ],
   "source": [
    "# ==================== ARIMA MODEL ====================\n",
    "\n",
    "class ARIMAModel:\n",
    "    \"\"\"ARIMA model for time series forecasting\"\"\"\n",
    "    \n",
    "    def __init__(self, order=(5, 1, 2)):\n",
    "        self.order = order\n",
    "        self.model = None\n",
    "        self.fitted_model = None\n",
    "        self.training_time = 0\n",
    "        self.predictions = []\n",
    "    \n",
    "    def fit(self, returns):\n",
    "        \"\"\"Fit ARIMA model\"\"\"\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        self.model = ARIMA(returns, order=self.order)\n",
    "        self.fitted_model = self.model.fit()\n",
    "        \n",
    "        self.training_time = time.time() - start_time\n",
    "        return self.fitted_model\n",
    "    \n",
    "    def predict(self, steps=1):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        forecast = self.fitted_model.get_forecast(steps=steps)\n",
    "        return forecast.predicted_mean.values\n",
    "    \n",
    "    def get_summary(self):\n",
    "        \"\"\"Get model summary\"\"\"\n",
    "        if self.fitted_model:\n",
    "            return self.fitted_model.summary()\n",
    "        return \"Model not fitted yet\"\n",
    "\n",
    "print(\"âœ“ ARIMAModel class created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fd872e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ SARIMAModel class created!\n"
     ]
    }
   ],
   "source": [
    "# ==================== SARIMA MODEL ====================\n",
    "\n",
    "class SARIMAModel:\n",
    "    \"\"\"Seasonal ARIMA model for seasonal time series\"\"\"\n",
    "    \n",
    "    def __init__(self, order=(1, 1, 1), seasonal_order=(1, 1, 1, 12)):\n",
    "        self.order = order\n",
    "        self.seasonal_order = seasonal_order\n",
    "        self.model = None\n",
    "        self.fitted_model = None\n",
    "        self.training_time = 0\n",
    "        self.predictions = []\n",
    "    \n",
    "    def fit(self, returns):\n",
    "        \"\"\"Fit SARIMA model\"\"\"\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        self.model = SARIMAX(returns, order=self.order, seasonal_order=self.seasonal_order)\n",
    "        self.fitted_model = self.model.fit(disp=False)\n",
    "        \n",
    "        self.training_time = time.time() - start_time\n",
    "        return self.fitted_model\n",
    "    \n",
    "    def predict(self, steps=1):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        forecast = self.fitted_model.get_forecast(steps=steps)\n",
    "        return forecast.predicted_mean.values\n",
    "    \n",
    "    def get_summary(self):\n",
    "        \"\"\"Get model summary\"\"\"\n",
    "        if self.fitted_model:\n",
    "            return self.fitted_model.summary()\n",
    "        return \"Model not fitted yet\"\n",
    "\n",
    "print(\"âœ“ SARIMAModel class created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dc548f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "VaR PREDICTION: DEEP LEARNING VS STATISTICAL MODELS\n",
      "MENA Region Stock Market Indices\n",
      "================================================================================\n",
      "\n",
      "Configuration:\n",
      "  Indices: ['Tunindex', 'ADI', 'MASI', 'TASI']\n",
      "  Lookback: 60\n",
      "  Epochs: 50\n",
      "  Data Path: C:\\Users\\sfaxi\\Desktop\\Deep Learning\\data\n"
     ]
    }
   ],
   "source": [
    "# ==================== MAIN EXECUTION SETUP ====================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"VaR PREDICTION: DEEP LEARNING VS STATISTICAL MODELS\")\n",
    "print(\"MENA Region Stock Market Indices\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Configuration\n",
    "indices = ['Tunindex', 'ADI', 'MASI', 'TASI']\n",
    "lookback = 60\n",
    "epochs = 50\n",
    "\n",
    "# Paths\n",
    "data_path = r'C:\\Users\\sfaxi\\Desktop\\Deep Learning\\data'\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Indices: {indices}\")\n",
    "print(f\"  Lookback: {lookback}\")\n",
    "print(f\"  Epochs: {epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7caeba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "LOADING & CLEANING DATA\n",
      "============================================================\n",
      "\n",
      "Processing ADI...\n",
      "  Loaded 2585 records\n",
      "âœ“ No missing values found\n",
      "âš  Detected 13 potential outliers (0.50%)\n",
      "  Outlier dates: 2005-03-02, 2007-10-21, 2008-01-22, 2009-11-30, 2011-01-30 ... and 8 more\n",
      "\n",
      "============================================================\n",
      "DATA SUMMARY: ADI\n",
      "============================================================\n",
      "Date Range: 2005-01-03 to 2014-12-31\n",
      "Total Records: 2585\n",
      "Missing Values: 0\n",
      "\n",
      "Price Statistics:\n",
      "  Min:    $2136.64\n",
      "  Max:    $6237.98\n",
      "  Mean:   $3588.18\n",
      "  Median: $3298.11\n",
      "  Std:    $1031.20\n",
      "\n",
      "Return Statistics (Log Returns):\n",
      "  Mean:   0.0131%\n",
      "  Std:    1.2701%\n",
      "  Min:    -8.6793%\n",
      "  Max:    7.6295%\n",
      "  Skew:   -0.0614\n",
      "  Kurt:   6.9896\n",
      "============================================================\n",
      "\n",
      "\n",
      "Processing CAC40...\n",
      "  Loaded 2560 records\n",
      "âœ“ No missing values found\n",
      "âš  Detected 9 potential outliers (0.35%)\n",
      "  Outlier dates: 2007-02-27, 2008-01-21, 2008-09-19, 2010-05-10, 2012-03-06 ... and 4 more\n",
      "\n",
      "============================================================\n",
      "DATA SUMMARY: CAC40\n",
      "============================================================\n",
      "Date Range: 2005-01-03 to 2014-12-31\n",
      "Total Records: 2560\n",
      "Missing Values: 0\n",
      "\n",
      "Price Statistics:\n",
      "  Min:    $2519.29\n",
      "  Max:    $6168.15\n",
      "  Mean:   $4174.61\n",
      "  Median: $4038.35\n",
      "  Std:    $797.36\n",
      "\n",
      "Return Statistics (Log Returns):\n",
      "  Mean:   0.0040%\n",
      "  Std:    1.4468%\n",
      "  Min:    -9.4715%\n",
      "  Max:    10.5946%\n",
      "  Skew:   0.0529\n",
      "  Kurt:   6.7784\n",
      "============================================================\n",
      "\n",
      "\n",
      "Processing MASI...\n",
      "  Loaded 2496 records\n",
      "âœ“ No missing values found\n",
      "âš  Detected 17 potential outliers (0.68%)\n",
      "  Outlier dates: 2005-02-23, 2005-02-24, 2005-08-25, 2006-08-08, 2007-05-09 ... and 12 more\n",
      "\n",
      "============================================================\n",
      "DATA SUMMARY: MASI\n",
      "============================================================\n",
      "Date Range: 2005-01-03 to 2014-12-31\n",
      "Total Records: 2496\n",
      "Missing Values: 0\n",
      "\n",
      "Price Statistics:\n",
      "  Min:    $4352.67\n",
      "  Max:    $14925.99\n",
      "  Mean:   $10093.30\n",
      "  Median: $10393.35\n",
      "  Std:    $2475.51\n",
      "\n",
      "Return Statistics (Log Returns):\n",
      "  Mean:   0.0303%\n",
      "  Std:    0.8302%\n",
      "  Min:    -5.0167%\n",
      "  Max:    4.4635%\n",
      "  Skew:   -0.3904\n",
      "  Kurt:   4.8935\n",
      "============================================================\n",
      "\n",
      "\n",
      "Processing S&P500...\n",
      "  Loaded 2536 records\n",
      "âœ“ No missing values found\n",
      "âš  Detected 5 potential outliers (0.20%)\n",
      "  Outlier dates: 2007-02-27, 2011-08-08, 2012-09-06, 2014-01-24, 2014-09-22\n",
      "\n",
      "============================================================\n",
      "DATA SUMMARY: S&P500\n",
      "============================================================\n",
      "Date Range: 2005-01-03 to 2014-12-31\n",
      "Total Records: 2536\n",
      "Missing Values: 0\n",
      "\n",
      "Price Statistics:\n",
      "  Min:    $676.00\n",
      "  Max:    $2085.75\n",
      "  Mean:   $1354.30\n",
      "  Median: $1314.50\n",
      "  Std:    $279.17\n",
      "\n",
      "Return Statistics (Log Returns):\n",
      "  Mean:   0.0210%\n",
      "  Std:    1.2974%\n",
      "  Min:    -10.4003%\n",
      "  Max:    13.2022%\n",
      "  Skew:   -0.1179\n",
      "  Kurt:   14.0981\n",
      "============================================================\n",
      "\n",
      "\n",
      "Processing TASI...\n",
      "  Loaded 2578 records\n",
      "âœ“ No missing values found\n",
      "âš  Detected 14 potential outliers (0.54%)\n",
      "  Outlier dates: 2006-02-26, 2006-07-15, 2010-04-20, 2010-05-08, 2011-01-29 ... and 9 more\n",
      "\n",
      "============================================================\n",
      "DATA SUMMARY: TASI\n",
      "============================================================\n",
      "Date Range: 2005-01-03 to 2014-12-31\n",
      "Total Records: 2578\n",
      "Missing Values: 0\n",
      "\n",
      "Price Statistics:\n",
      "  Min:    $4130.01\n",
      "  Max:    $20634.86\n",
      "  Mean:   $8567.44\n",
      "  Median: $7593.16\n",
      "  Std:    $3011.33\n",
      "\n",
      "Return Statistics (Log Returns):\n",
      "  Mean:   0.0009%\n",
      "  Std:    1.6852%\n",
      "  Min:    -10.3285%\n",
      "  Max:    9.3907%\n",
      "  Skew:   -0.9023\n",
      "  Kurt:   8.2190\n",
      "============================================================\n",
      "\n",
      "\n",
      "Processing Tunindex...\n",
      "  Loaded 2471 records\n",
      "âœ“ No missing values found\n",
      "âš  Detected 17 potential outliers (0.69%)\n",
      "  Outlier dates: 2005-04-06, 2005-05-31, 2008-03-31, 2008-08-11, 2008-10-06 ... and 12 more\n",
      "\n",
      "============================================================\n",
      "DATA SUMMARY: Tunindex\n",
      "============================================================\n",
      "Date Range: 2005-01-03 to 2014-12-31\n",
      "Total Records: 2471\n",
      "Missing Values: 0\n",
      "\n",
      "Price Statistics:\n",
      "  Min:    $1304.78\n",
      "  Max:    $5681.39\n",
      "  Mean:   $3623.76\n",
      "  Median: $4146.11\n",
      "  Std:    $1247.34\n",
      "\n",
      "Return Statistics (Log Returns):\n",
      "  Mean:   0.0542%\n",
      "  Std:    0.5868%\n",
      "  Min:    -5.0015%\n",
      "  Max:    4.1086%\n",
      "  Skew:   -0.5443\n",
      "  Kurt:   11.6434\n",
      "============================================================\n",
      "\n",
      "\n",
      "âœ“ Successfully loaded and cleaned 6 datasets\n"
     ]
    }
   ],
   "source": [
    "# ==================== DATA LOADING & INITIAL CLEANING ====================\n",
    "\n",
    "preprocessor = DataPreprocessor(lookback=60)\n",
    "\n",
    "datasets = {\n",
    "    'ADI': 'data/ADI.csv',\n",
    "    'CAC40': 'data/CAC40.csv',\n",
    "    'MASI': 'data/MASI.csv',\n",
    "    'S&P500': 'data/S&P500.csv',\n",
    "    'TASI': 'data/TASI.csv',\n",
    "    'Tunindex': 'data/Tunindex.csv'\n",
    "}\n",
    "\n",
    "all_data = {}\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LOADING & CLEANING DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for name, filepath in datasets.items():\n",
    "    try:\n",
    "        print(f\"\\nProcessing {name}...\")\n",
    "        df = preprocessor.load_data(filepath)\n",
    "        print(f\"  Loaded {len(df)} records\")\n",
    "        \n",
    "        df = preprocessor.handle_missing_values(df, method='forward_fill')\n",
    "        df = preprocessor.detect_outliers(df, window=20, std_threshold=3)\n",
    "        preprocessor.get_data_summary(df, name)\n",
    "        \n",
    "        all_data[name] = df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— Error processing {name}: {str(e)}\")\n",
    "\n",
    "print(f\"\\nâœ“ Successfully loaded and cleaned {len(all_data)} datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c739fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Processing: Tunindex\n",
      "================================================================================\n",
      "\n",
      "Data shape: (2471, 7)\n",
      "Date range: 2005-01-03 00:00:00 to 2014-12-31 00:00:00\n",
      "\n",
      "Training set size: 1928\n",
      "Test set size: 482\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==================== PROCESS SINGLE INDEX ====================\n",
    "\n",
    "index = 'Tunindex'\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(f\"Processing: {index}\")\n",
    "print(f\"{'=' * 80}\\n\")\n",
    "\n",
    "try:\n",
    "    preprocessor = DataPreprocessor(lookback=lookback)\n",
    "    df = preprocessor.load_data(rf\"{data_path}\\{index}.csv\")\n",
    "    \n",
    "    print(f\"Data shape: {df.shape}\")\n",
    "    print(f\"Date range: {df['Date'].min()} to {df['Date'].max()}\\n\")\n",
    "    \n",
    "    X, y, returns_scaled = preprocessor.preprocess_for_deeplearning(df)\n",
    "    X_train, X_test, y_train, y_test = preprocessor.split_train_test(X, y)\n",
    "    \n",
    "    print(f\"Training set size: {X_train.shape[0]}\")\n",
    "    print(f\"Test set size: {X_test.shape[0]}\\n\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797f58e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ANN Model...\n",
      "\n",
      "ANN Metrics:\n",
      "  MAE: 0.031530\n",
      "  RMSE: 0.047225\n",
      "  MAPE: 6.214191\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==================== TRAIN ANN MODEL ====================\n",
    "\n",
    "print(\"Training ANN Model...\")\n",
    "ann_model = ANNModel(input_shape=X_train.shape[1] * X_train.shape[2])\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "history_ann = ann_model.train(X_train_flat, y_train, epochs=epochs)\n",
    "y_pred_ann = ann_model.predict(X_test_flat).flatten()\n",
    "metrics_ann = ModelEvaluator.calculate_metrics(y_test, y_pred_ann)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ANN MODEL EVALUATION\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  MAE:  {metrics_ann['MAE']:.8f}\")\n",
    "print(f\"  RMSE: {metrics_ann['RMSE']:.8f}\")\n",
    "print(f\"  MAPE: {metrics_ann['MAPE']:.4f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13037ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LSTM Model...\n",
      "\n",
      "LSTM Metrics:\n",
      "  MAE: 0.032376\n",
      "  RMSE: 0.048255\n",
      "  MAPE: 6.362080\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==================== TRAIN LSTM MODEL ====================\n",
    "\n",
    "print(\"Training LSTM Model...\")\n",
    "lstm_model = LSTMModel(input_shape=(X_train.shape[1], X_train.shape[2]))\n",
    "history_lstm = lstm_model.train(X_train, y_train, epochs=epochs)\n",
    "y_pred_lstm = lstm_model.predict(X_test).flatten()\n",
    "metrics_lstm = ModelEvaluator.calculate_metrics(y_test, y_pred_lstm)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"LSTM MODEL EVALUATION\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  MAE:  {metrics_lstm['MAE']:.8f}\")\n",
    "print(f\"  RMSE: {metrics_lstm['RMSE']:.8f}\")\n",
    "print(f\"  MAPE: {metrics_lstm['MAPE']:.4f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e73ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CNN Model...\n",
      "\n",
      "CNN Metrics:\n",
      "  MAE: 0.031638\n",
      "  RMSE: 0.047517\n",
      "  MAPE: 6.196648\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==================== TRAIN CNN MODEL ====================\n",
    "\n",
    "print(\"Training CNN Model...\")\n",
    "cnn_model = CNNModel(input_shape=(X_train.shape[1], X_train.shape[2]))\n",
    "history_cnn = cnn_model.train(X_train, y_train, epochs=epochs)\n",
    "y_pred_cnn = cnn_model.predict(X_test).flatten()\n",
    "metrics_cnn = ModelEvaluator.calculate_metrics(y_test, y_pred_cnn)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"CNN MODEL EVALUATION\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  MAE:  {metrics_cnn['MAE']:.8f}\")\n",
    "print(f\"  RMSE: {metrics_cnn['RMSE']:.8f}\")\n",
    "print(f\"  MAPE: {metrics_cnn['MAPE']:.4f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f42751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ARIMA Model...\n",
      "\n",
      "ARIMA Metrics:\n",
      "  MAE: 0.050179\n",
      "  RMSE: 0.050363\n",
      "  MAPE: 18540.142820\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==================== TRAIN ARIMA MODEL ====================\n",
    "\n",
    "print(\"Training ARIMA Model...\")\n",
    "returns = preprocessor.calculate_returns(df['Price'])\n",
    "arima_model = ARIMAModel(order=(5, 1, 2))\n",
    "arima_model.fit(returns)\n",
    "\n",
    "y_pred_arima = np.array([arima_model.predict(steps=1)[0] for _ in range(len(y_test))])\n",
    "y_pred_arima_denorm = preprocessor.scaler.inverse_transform(y_pred_arima.reshape(-1, 1)).flatten()\n",
    "y_test_denorm = preprocessor.scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "metrics_arima = ModelEvaluator.calculate_metrics(y_test_denorm, y_pred_arima_denorm)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ARIMA MODEL EVALUATION\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  MAE:  {metrics_arima['MAE']:.8f}\")\n",
    "print(f\"  RMSE: {metrics_arima['RMSE']:.8f}\")\n",
    "print(f\"  MAPE: {metrics_arima['MAPE']:.4f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2187a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating VaR...\n",
      "\n",
      "Historical VaR:\n",
      "  VaR (95%): -0.050668\n",
      "  VaR (99%): -0.051616\n",
      "\n",
      "LSTM Predicted VaR:\n",
      "  VaR (95%): 0.532171\n",
      "  VaR (99%): 0.526482\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==================== CALCULATE VAR ====================\n",
    "\n",
    "print(\"Calculating VaR...\")\n",
    "returns_denorm = preprocessor.scaler.inverse_transform(returns.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "var_95 = np.percentile(returns_denorm, 5)\n",
    "var_99 = np.percentile(returns_denorm, 1)\n",
    "\n",
    "var_95_predicted_lstm = np.percentile(y_pred_lstm, 5)\n",
    "var_99_predicted_lstm = np.percentile(y_pred_lstm, 1)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"VALUE AT RISK (VaR) CALCULATION\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Historical VaR (95%): {var_95:.6f}\")\n",
    "print(f\"Historical VaR (99%): {var_99:.6f}\")\n",
    "print(f\"\\nLSTM Predicted VaR (95%): {var_95_predicted_lstm:.6f}\")\n",
    "print(f\"LSTM Predicted VaR (99%): {var_99_predicted_lstm:.6f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3672f835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VaR Backtesting Results:\n",
      "\n",
      "95% Confidence Level:\n",
      "  Exceptions: 25\n",
      "  Expected Exceptions: 24.1\n",
      "  Exception Rate: 0.0519\n",
      "\n",
      "99% Confidence Level:\n",
      "  Exceptions: 5\n",
      "  Expected Exceptions: 4.8\n",
      "  Exception Rate: 0.0104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==================== VAR BACKTESTING ====================\n",
    "\n",
    "print(\"VaR Backtesting...\")\n",
    "backtest_95_exceptions = np.sum(y_test < np.percentile(y_test, 5))\n",
    "backtest_99_exceptions = np.sum(y_test < np.percentile(y_test, 1))\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"BACKTESTING RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"95% Confidence Level:\")\n",
    "print(f\"  Exceptions: {backtest_95_exceptions}\")\n",
    "print(f\"  Expected: {int(0.05 * len(y_test))}\")\n",
    "print(f\"\\n99% Confidence Level:\")\n",
    "print(f\"  Exceptions: {backtest_99_exceptions}\")\n",
    "print(f\"  Expected: {int(0.01 * len(y_test))}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d6f122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== STORE RESULTS ====================\n",
    "\n",
    "results[index] = {\n",
    "    'ANN': metrics_ann,\n",
    "    'LSTM': metrics_lstm,\n",
    "    'CNN': metrics_cnn,\n",
    "    'ARIMA': metrics_arima,\n",
    "    'VaR_95': var_95,\n",
    "    'VaR_99': var_99,\n",
    "    'VaR_95_LSTM': var_95_predicted_lstm,\n",
    "    'VaR_99_LSTM': var_99_predicted_lstm\n",
    "}\n",
    "\n",
    "print(f\"Results stored for {index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb51b143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== COMPREHENSIVE MODEL EVALUATION TABLE ====================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 140)\n",
    "print(\"COMPREHENSIVE MODEL EVALUATION - ALL MODELS\")\n",
    "print(\"=\" * 140 + \"\\n\")\n",
    "\n",
    "# Create evaluation summary\n",
    "evaluation_data = {\n",
    "    'Model': ['ANN', 'LSTM', 'CNN', 'ARIMA'],\n",
    "    'MAE': [\n",
    "        metrics_ann['MAE'],\n",
    "        metrics_lstm['MAE'],\n",
    "        metrics_cnn['MAE'],\n",
    "        metrics_arima['MAE']\n",
    "    ],\n",
    "    'RMSE': [\n",
    "        metrics_ann['RMSE'],\n",
    "        metrics_lstm['RMSE'],\n",
    "        metrics_cnn['RMSE'],\n",
    "        metrics_arima['RMSE']\n",
    "    ],\n",
    "    'MAPE (%)': [\n",
    "        metrics_ann['MAPE'],\n",
    "        metrics_lstm['MAPE'],\n",
    "        metrics_cnn['MAPE'],\n",
    "        metrics_arima['MAPE']\n",
    "    ],\n",
    "    'Type': ['Deep Learning', 'Deep Learning', 'Deep Learning', 'Statistical'],\n",
    "    'Parameters': ['~7,000', '~8,500', '~15,000+', '~50'],\n",
    "    'Training Time (s)': [\n",
    "        ann_model.training_time,\n",
    "        lstm_model.training_time,\n",
    "        cnn_model.training_time,\n",
    "        arima_model.training_time\n",
    "    ]\n",
    "}\n",
    "\n",
    "eval_df = pd.DataFrame(evaluation_data)\n",
    "\n",
    "# Display table\n",
    "print(eval_df.to_string(index=False))\n",
    "\n",
    "# Rankings\n",
    "print(f\"\\n\" + \"=\" * 140)\n",
    "print(\"RANKINGS BY PERFORMANCE\")\n",
    "print(\"=\" * 140 + \"\\n\")\n",
    "\n",
    "print(\"ðŸ† BEST ACCURACY (Lowest MAE):\")\n",
    "best_mae_idx = eval_df['MAE'].idxmin()\n",
    "print(f\"   {eval_df.loc[best_mae_idx, 'Model']}: {eval_df.loc[best_mae_idx, 'MAE']:.8f}\\n\")\n",
    "\n",
    "print(\"âš¡ FASTEST TRAINING:\")\n",
    "fastest_idx = eval_df['Training Time (s)'].idxmin()\n",
    "print(f\"   {eval_df.loc[fastest_idx, 'Model']}: {eval_df.loc[fastest_idx, 'Training Time (s)']:.3f}s\\n\")\n",
    "\n",
    "print(\"ðŸ“Š BEST PERCENTAGE ERROR (Lowest MAPE):\")\n",
    "best_mape_idx = eval_df['MAPE (%)'].idxmin()\n",
    "print(f\"   {eval_df.loc[best_mape_idx, 'Model']}: {eval_df.loc[best_mape_idx, 'MAPE (%)']:.4f}%\\n\")\n",
    "\n",
    "# Type comparison\n",
    "dl_data = eval_df[eval_df['Type'] == 'Deep Learning']['MAE']\n",
    "stat_data = eval_df[eval_df['Type'] == 'Statistical']['MAE']\n",
    "\n",
    "print(\"DEEP LEARNING VS STATISTICAL:\")\n",
    "print(f\"   Deep Learning Avg MAE: {dl_data.mean():.8f}\")\n",
    "print(f\"   Statistical Avg MAE:   {stat_data.mean():.8f}\")\n",
    "print(f\"   Winner: {eval_df.loc[eval_df['MAE'].idxmin(), 'Type']}\\n\")\n",
    "\n",
    "print(\"=\" * 140 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== VISUAL MODEL COMPARISON ====================\n",
    "\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.35, wspace=0.3)\n",
    "\n",
    "# 1. MAE Comparison\n",
    "ax1 = fig.add_subplot(gs[0, :2])\n",
    "colors_mae = ['#2ecc71' if x == eval_df['MAE'].min() else '#3498db' for x in eval_df['MAE']]\n",
    "bars1 = ax1.bar(eval_df['Model'], eval_df['MAE'], color=colors_mae, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "ax1.set_ylabel('MAE', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Model Accuracy Comparison (MAE)', fontsize=13, fontweight='bold')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "for i, bar in enumerate(bars1):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f' {v:.6f}', va='center', fontweight='bold')\n",
    "\n",
    "# 2. Best Model Indicator\n",
    "ax2 = fig.add_subplot(gs[0, 2])\n",
    "ax2.axis('off')\n",
    "best_model = eval_df.loc[eval_df['MAE'].idxmin(), 'Model']\n",
    "best_mae = eval_df['MAE'].min()\n",
    "ax2.text(0.5, 0.7, 'ðŸ† BEST MODEL', ha='center', fontsize=14, fontweight='bold', transform=ax2.transAxes)\n",
    "ax2.text(0.5, 0.45, best_model, ha='center', fontsize=16, fontweight='bold', color='#2ecc71', transform=ax2.transAxes)\n",
    "ax2.text(0.5, 0.2, f'MAE: {best_mae:.8f}', ha='center', fontsize=11, transform=ax2.transAxes)\n",
    "\n",
    "# 3. RMSE Comparison\n",
    "ax3 = fig.add_subplot(gs[1, 0])\n",
    "colors_rmse = ['#e74c3c' if x == eval_df['RMSE'].min() else '#e67e22' for x in eval_df['RMSE']]\n",
    "bars3 = ax3.bar(eval_df['Model'], eval_df['RMSE'], color=colors_rmse, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "ax3.set_ylabel('RMSE', fontsize=11, fontweight='bold')\n",
    "ax3.set_title('Stability (RMSE)', fontsize=12, fontweight='bold')\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "for i, bar in enumerate(bars3):\n",
    "    height = bar.get_height()\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{eval_df.iloc[i][\"RMSE\"]:.4f}', ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "\n",
    "# 4. MAPE Comparison\n",
    "ax4 = fig.add_subplot(gs[1, 1])\n",
    "colors_mape = ['#9b59b6' if x == eval_df['MAPE (%)'].min() else '#af7ac5' for x in eval_df['MAPE (%)']]\n",
    "bars4 = ax4.bar(eval_df['Model'], eval_df['MAPE (%)'], color=colors_mape, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "ax4.set_ylabel('MAPE (%)', fontsize=11, fontweight='bold')\n",
    "ax4.set_title('Percentage Error (MAPE)', fontsize=12, fontweight='bold')\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "for i, bar in enumerate(bars4):\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{eval_df.iloc[i][\"MAPE (%)\"]:.2f}%', ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "\n",
    "# 5. Training Time\n",
    "ax5 = fig.add_subplot(gs[1, 2])\n",
    "colors_time = ['#1abc9c' if x == eval_df['Training Time (s)'].min() else '#16a085' for x in eval_df['Training Time (s)']]\n",
    "bars5 = ax5.bar(eval_df['Model'], eval_df['Training Time (s)'], color=colors_time, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "ax5.set_ylabel('Time (s)', fontsize=11, fontweight='bold')\n",
    "ax5.set_title('Training Speed', fontsize=12, fontweight='bold')\n",
    "ax5.grid(axis='y', alpha=0.3)\n",
    "for i, bar in enumerate(bars5):\n",
    "    height = bar.get_height()\n",
    "    ax5.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{eval_df.iloc[i][\"Training Time (s)\"]:.2f}s', ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "\n",
    "# 6. Type Distribution\n",
    "ax6 = fig.add_subplot(gs[2, 0])\n",
    "type_counts = eval_df['Type'].value_counts()\n",
    "colors_type = ['#3498db', '#e74c3c']\n",
    "wedges, texts, autotexts = ax6.pie(type_counts.values, labels=type_counts.index, autopct='%1.0f%%',\n",
    "                                     colors=colors_type, startangle=90, textprops={'fontsize': 10, 'fontweight': 'bold'})\n",
    "ax6.set_title('Model Types', fontsize=12, fontweight='bold')\n",
    "\n",
    "# 7. Detailed Metrics Table\n",
    "ax7 = fig.add_subplot(gs[2, 1:])\n",
    "ax7.axis('tight')\n",
    "ax7.axis('off')\n",
    "\n",
    "table_data = []\n",
    "for idx, row in eval_df.iterrows():\n",
    "    table_data.append([\n",
    "        row['Model'],\n",
    "        f\"{row['MAE']:.8f}\",\n",
    "        f\"{row['RMSE']:.8f}\",\n",
    "        f\"{row['MAPE (%)']:.4f}%\",\n",
    "        row['Type'],\n",
    "        f\"{row['Training Time (s)']:.3f}s\"\n",
    "    ])\n",
    "\n",
    "table = ax7.table(cellText=table_data,\n",
    "                  colLabels=['Model', 'MAE', 'RMSE', 'MAPE', 'Type', 'Training Time'],\n",
    "                  cellLoc='center',\n",
    "                  loc='center',\n",
    "                  colWidths=[0.12, 0.18, 0.18, 0.15, 0.18, 0.15])\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1, 2.5)\n",
    "\n",
    "# Style header\n",
    "for i in range(6):\n",
    "    table[(0, i)].set_facecolor('#34495e')\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "# Style rows\n",
    "for i in range(1, len(table_data) + 1):\n",
    "    for j in range(6):\n",
    "        if i % 2 == 0:\n",
    "            table[(i, j)].set_facecolor('#ecf0f1')\n",
    "        else:\n",
    "            table[(i, j)].set_facecolor('#ffffff')\n",
    "        table[(i, j)].set_edgecolor('#bdc3c7')\n",
    "        table[(i, j)].set_linewidth(1.5)\n",
    "\n",
    "ax7.set_title('Detailed Model Metrics', fontsize=12, fontweight='bold', pad=20)\n",
    "\n",
    "# Main title\n",
    "fig.suptitle(f'Model Evaluation Summary - {index}', fontsize=16, fontweight='bold', y=0.98)\n",
    "\n",
    "plt.savefig('model_evaluation_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Comprehensive evaluation image saved as 'model_evaluation_comparison.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd2ac84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== FINAL RECOMMENDATIONS ====================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 140)\n",
    "print(\"FINAL RECOMMENDATIONS & CONCLUSIONS\")\n",
    "print(\"=\" * 140 + \"\\n\")\n",
    "\n",
    "best_model_name = eval_df.loc[eval_df['MAE'].idxmin(), 'Model']\n",
    "best_metrics = eval_df.loc[eval_df['MAE'].idxmin()]\n",
    "\n",
    "print(f\"âœ“ BEST PERFORMING MODEL: {best_model_name}\")\n",
    "print(f\"  â””â”€ MAE:  {best_metrics['MAE']:.8f}\")\n",
    "print(f\"  â””â”€ RMSE: {best_metrics['RMSE']:.8f}\")\n",
    "print(f\"  â””â”€ MAPE: {best_metrics['MAPE (%)']:.4f}%\")\n",
    "print(f\"  â””â”€ Training Time: {best_metrics['Training Time (s)']:.3f}s\\n\")\n",
    "\n",
    "print(\"ðŸ“Œ MODEL-SPECIFIC INSIGHTS:\\n\")\n",
    "\n",
    "print(f\"ANN (Artificial Neural Network)\")\n",
    "print(f\"  â€¢ MAE: {metrics_ann['MAE']:.8f}\")\n",
    "print(f\"  â€¢ Rank: #{eval_df[eval_df['Model'] == 'ANN'].index[0] + 1}\")\n",
    "print(f\"  â€¢ Best for: Non-linear pattern recognition\\n\")\n",
    "\n",
    "print(f\"LSTM (Long Short-Term Memory)\")\n",
    "print(f\"  â€¢ MAE: {metrics_lstm['MAE']:.8f}\")\n",
    "print(f\"  â€¢ Rank: #{eval_df[eval_df['Model'] == 'LSTM'].index[0] + 1}\")\n",
    "print(f\"  â€¢ Best for: Temporal dependencies and sequential patterns\\n\")\n",
    "\n",
    "print(f\"CNN (Convolutional Neural Network)\")\n",
    "print(f\"  â€¢ MAE: {metrics_cnn['MAE']:.8f}\")\n",
    "print(f\"  â€¢ Rank: #{eval_df[eval_df['Model'] == 'CNN'].index[0] + 1}\")\n",
    "print(f\"  â€¢ Best for: Feature extraction from time series\\n\")\n",
    "\n",
    "print(f\"ARIMA (AutoRegressive Integrated Moving Average)\")\n",
    "print(f\"  â€¢ MAE: {metrics_arima['MAE']:.8f}\")\n",
    "print(f\"  â€¢ Rank: #{eval_df[eval_df['Model'] == 'ARIMA'].index[0] + 1}\")\n",
    "print(f\"  â€¢ Best for: Interpretable, lightweight forecasting\\n\")\n",
    "\n",
    "print(\"ðŸ’¡ DEPLOYMENT RECOMMENDATIONS:\\n\")\n",
    "print(\"1ï¸âƒ£  For Production (Accuracy Priority):\")\n",
    "print(f\"    â†’ Use {best_model_name} model\")\n",
    "print(f\"    â†’ Expected MAE: {best_metrics['MAE']:.8f}\\n\")\n",
    "\n",
    "print(\"2ï¸âƒ£  For Regulatory Compliance:\")\n",
    "print(\"    â†’ Use ARIMA (highly interpretable)\")\n",
    "print(\"    â†’ Easy to document and explain\\n\")\n",
    "\n",
    "print(\"3ï¸âƒ£  For Hybrid Approach:\")\n",
    "print(\"    â†’ Ensemble: Average predictions from top 2 models\")\n",
    "print(\"    â†’ Reduces model-specific bias\\n\")\n",
    "\n",
    "print(\"=\" * 140 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
